{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e97c4a-a26b-498a-881e-e95235f91b31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pip in /root/miniconda3/lib/python3.10/site-packages (24.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Invalid requirement: 'rags=0.1.11': Expected end or semicolon (after name and no valid version specifier)\n",
      "    rags=0.1.11\n",
      "        ^ (from line 14 of requirements.txt)\n",
      "Hint: = is not a valid operator. Did you mean == ?\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pip\n",
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d8bc2b-4ac8-47e8-8d77-48f568feaecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57429ba8-0afd-44e0-98ea-c552c9258723",
   "metadata": {},
   "outputs": [],
   "source": [
    "DESCRIPTION = '''\n",
    "<div>\n",
    "<h1 style=\"text-align: center;\">EmoLLM 心理咨询室</h1>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <a href=\"https://github.com/Rvosuke/EmoLLM/\">\n",
    "  </a>\n",
    "</p>\n",
    "\n",
    "</div>\n",
    "'''\n",
    "\n",
    "LICENSE = \"\"\"\n",
    "<p align=\"center\"> Built with 26组 白泽阳 & 钱兴宇 </>\n",
    "\"\"\"\n",
    "\n",
    "PLACEHOLDER = \"\"\"\n",
    "<div style=\"padding: 30px; text-align: center; display: flex; flex-direction: column; align-items: center;\">\n",
    "\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "css = \"\"\"\n",
    "h1 {\n",
    "  text-align: center;\n",
    "  display: block;\n",
    "}\n",
    "<!--\n",
    "#duplicate-button {\n",
    "  margin: auto;\n",
    "  color: white;\n",
    "  background: #1565c0;\n",
    "  border-radius: 100vh;\n",
    "}\n",
    "-->\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66864abc-8224-4be6-ac4c-afb7ba5f0c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:52<00:00, 13.04s/it]\n"
     ]
    }
   ],
   "source": [
    "# download internlm2 to the base_path directory using git tool\n",
    "base_path = os.path.expandvars('$GEMINI_PRETRAIN2')\n",
    "# os.system(f'git clone https://code.openxlab.org.cn/chg0901/EmoLLM-Llama3-8B-Instruct3.0.git {base_path}')\n",
    "# os.system(f'cd {base_path} && git lfs pull')\n",
    "\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path,trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_path,trust_remote_code=True, device_map=\"auto\", torch_dtype=torch.float16).eval()  # to(\"cuda:0\") \n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "def chat_llama3_8b(message: str, \n",
    "              history: list, \n",
    "              temperature: float, \n",
    "              max_new_tokens: int,\n",
    "              top_p: float\n",
    "             ) -> str:\n",
    "    \"\"\"\n",
    "    Generate a streaming response using the llama3-8b model.\n",
    "    Args:\n",
    "        message (str): The input message.\n",
    "        history (list): The conversation history used by ChatInterface.\n",
    "        temperature (float): The temperature for generating the response.\n",
    "        max_new_tokens (int): The maximum number of new tokens to generate.\n",
    "    Returns:\n",
    "        str: The generated response.\n",
    "    \"\"\"\n",
    "    conversation = []\n",
    "    message += \"你是一个心理咨询师，请使用中文来解决各种心理健康问题。注意循序渐进地回复，一次不要回复太多。\"\n",
    "    \n",
    "    for user, assistant in history:\n",
    "        conversation.extend([{\"role\": \"user\", \"content\": user}, {\"role\": \"assistant\", \"content\": assistant}])\n",
    "    conversation.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(conversation, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    generate_kwargs = dict(\n",
    "        input_ids= input_ids,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p = top_p,\n",
    "        eos_token_id=terminators,\n",
    "    )\n",
    "    # This will enforce greedy generation (do_sample=False) when the temperature is passed 0, avoiding the crash.             \n",
    "    if temperature == 0:\n",
    "        generate_kwargs['do_sample'] = False\n",
    "        \n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    outputs = []\n",
    "    for text in streamer:\n",
    "        outputs.append(text)\n",
    "        yield \"\".join(outputs)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f43e351c-4e56-4005-aa9a-6e08ee7adb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio block\n",
    "# , placeholder=PLACEHOLDER\n",
    "chatbot=gr.Chatbot(height=450, label='EmoLLM Chat')\n",
    "\n",
    "with gr.Blocks(css=css) as demo:\n",
    "    \n",
    "    gr.Markdown(DESCRIPTION)\n",
    "    # gr.DuplicateButton(value=\"Duplicate Space for private use\", elem_id=\"duplicate-button\")\n",
    "    gr.ChatInterface(\n",
    "        fn=chat_llama3_8b,\n",
    "        chatbot=chatbot,\n",
    "        additional_inputs_accordion=gr.Accordion(label=\"⚙️ Parameters\", open=False, render=False),\n",
    "        additional_inputs=[\n",
    "            gr.Slider(minimum=0,\n",
    "                      maximum=1, \n",
    "                      step=0.1,\n",
    "                      value=0.95, \n",
    "                      label=\"Temperature\", \n",
    "                      render=False),\n",
    "            gr.Slider(minimum=128, \n",
    "                      maximum=4096,\n",
    "                      step=1,\n",
    "                      value=4096, \n",
    "                      label=\"Max new tokens\", \n",
    "                      render=False ),\n",
    "            gr.Slider(minimum=0.0, \n",
    "                      maximum=1,\n",
    "                      step=0.01,\n",
    "                      value=0.8, \n",
    "                      label=\"Top P\", \n",
    "                      render=False ),\n",
    "            # gr.Slider(minimum=128, \n",
    "            #           maximum=4096,\n",
    "            #           step=1,\n",
    "            #           value=512, \n",
    "            #           label=\"Max new tokens\", \n",
    "            #           render=False ),\n",
    "            ],\n",
    "        examples=[\n",
    "            ['请介绍你自己。'],\n",
    "            ['我觉得我在学校的学习压力好大啊，虽然我真的很喜欢我的专业，但最近总是担心自己无法达到自己的期望，这让我有点焦虑。'],\n",
    "            ['我最近总觉得自己在感情上陷入了困境，我喜欢上了我的朋友，但又害怕表达出来会破坏我们现在的关系...'],\n",
    "            ['我感觉自己像是被困在一个无尽的循环中。每天醒来都感到身体沉重，对日常活动提不起兴趣，工作、锻炼甚至是我曾经喜欢的事物都让我觉得厌倦'],\n",
    "            ['最近工作压力特别大，还有一些家庭矛盾']\n",
    "            ],\n",
    "        cache_examples=False,\n",
    "                     )\n",
    "    \n",
    "    gr.Markdown(LICENSE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd5030d-9274-42f2-a65f-112b1890b815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "IMPORTANT: You are using gradio version 4.7.1, however version 4.29.0 is available, please upgrade.\n",
      "--------\n",
      "IMPORTANT: You are using gradio version 4.7.1, however version 4.29.0 is available, please upgrade.\n",
      "--------\n",
      "Running on public URL: https://ccd93ccffa96fdcb35.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ccd93ccffa96fdcb35.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01cd61f-4bad-4ca2-859f-b56dc137a661",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
