datasets==2.16.1
deepspeed==0.13.1
einops==0.7.0
openxlab==0.0.34
peft==0.7.1
sentencepiece==0.1.99
torch==2.1.2
transformers==4.36.2


# RAG
langchain==0.2.11
openai==1.37.0
rags=0.1.11
arxiv==2.1.3
pymupdf==1.24.9
chromadb==0.5.5
wandb==0.17.5 
tiktoken==0.7.0
pypdf==4.3.1
sentence_transformers==2.7.0
rank_bm25

# modified version
# xtuner==0.1.11
# mmengine==0.10.2
mmengine==0.10.3
xtuner==0.1.15

# flash_attn==2.5.0  # build is very slow about 2 hours?

# method 1: https://github.com/Dao-AILab/flash-attention/releases
# flash_attn-2.5.0+cu122torch2.1cxx11abiTRUE-cp310-cp310-linux_x86_64.whl
# method 2:
# pip install /root/share/wheels/flash_attn-2.4.2+cu118torch2.0cxx11abiTRUE-cp310-cp310-linux_x86_64.whl
