{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b89f64d8f8053d",
   "metadata": {
    "collapsed": false,
    "id": "89b89f64d8f8053d",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 任务一、基于ChatGLM3-6B模型进行LORA微调训练\n",
    "\n",
    "本案例基于 `AdvertiseGen` 数据集，对基座模型 ChatGLM3-6B 进行 lora微调，微调后的模型能够根据用户指令，生成广告文案。\n",
    "\n",
    "#### 硬件需求\n",
    "##### 运行本案例所需的硬件配置：\n",
    "\n",
    "1. 显存：24GB及以上（推荐使用30系或A10等sm80架构以上的NVIDIA显卡进行尝试）实际使用GPU RAM: 15.5/16.0 GB\n",
    "2. 内存：16GB以上\n",
    "\n",
    "##### 不同微调方式的硬件需求：\n",
    "\n",
    "1. SFT 全量微调: 4张显卡平均分配，每张显卡占用 48346MiB 显存。\n",
    "2. P-TuningV2 微调: 1张显卡，占用 18426MiB 显存。\n",
    "3. LORA 微调: 1张显卡，占用 14082MiB 显存。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd9a514ed09ea6",
   "metadata": {
    "collapsed": false,
    "id": "a7bd9a514ed09ea6",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 0. 环境检查\n",
    "首先，先检查代码的运行地址，确保运行地址处于 `finetune_demo` 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7703109d1443346",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T05:29:22.200365Z",
     "start_time": "2024-04-14T05:29:22.080929Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gemini/code/finetune\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed4dd3-9abf-4f60-90f1-45ecc6acd7f7",
   "metadata": {},
   "source": [
    "安装 `requirements.txt`中的依赖。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d1b1d9-c94b-4542-98b9-60df35e15c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.virtaicloud.com/repository/pypi/simple\n",
      "Collecting jieba>=0.42.1 (from -r requirements.txt (line 1))\n",
      "  Downloading https://pypi.virtaicloud.com/repository/pypi/packages/jieba/0.42.1/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m137.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ruamel_yaml>=0.18.6 (from -r requirements.txt (line 2))\n",
      "  Downloading https://pypi.virtaicloud.com/repository/pypi/packages/ruamel-yaml/0.18.6/ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m197.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rouge_chinese>=1.0.3 (from -r requirements.txt (line 3))\n",
      "  Downloading https://pypi.virtaicloud.com/repository/pypi/packages/rouge-chinese/1.0.3/rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "Collecting jupyter>=1.0.0 (from -r requirements.txt (line 4))\n",
      "  Downloading https://pypi.virtaicloud.com/repository/pypi/packages/jupyter/1.0.0/jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
      "Requirement already satisfied: datasets>=2.18.0 in /root/miniconda3/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (2.18.0)\n",
      "Collecting peft>=0.10.0 (from -r requirements.txt (line 6))\n",
      "  Downloading https://pypi.virtaicloud.com/repository/pypi/packages/peft/0.12.0/peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m239.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting deepspeed==0.13.1 (from -r requirements.txt (line 7))\n",
      "  Downloading https://pypi.virtaicloud.com/repository/pypi/packages/deepspeed/0.13.1/deepspeed-0.13.1.tar.gz (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m175.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: hjson in /root/miniconda3/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (3.1.0)\n",
      "Requirement already satisfied: ninja in /root/miniconda3/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (1.11.1.1)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (23.2)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (5.9.8)\n",
      "Requirement already satisfied: py-cpuinfo in /root/miniconda3/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (9.0.0)\n",
      "Requirement already satisfied: pydantic in /root/miniconda3/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (2.7.1)\n",
      "Requirement already satisfied: pynvml in /root/miniconda3/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (11.5.0)\n",
      "Requirement already satisfied: torch in /root/miniconda3/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (2.1.2)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (4.65.0)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel_yaml>=0.18.6->-r requirements.txt (line 2))\n",
      "  Downloading https://pypi.virtaicloud.com/repository/pypi/packages/ruamel-yaml-clib/0.2.8/ruamel.yaml.clib-0.2.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (544 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m544.0/544.0 kB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /root/miniconda3/lib/python3.11/site-packages (from rouge_chinese>=1.0.3->-r requirements.txt (line 3)) (1.16.0)\n",
      "Collecting notebook (from jupyter>=1.0.0->-r requirements.txt (line 4))\n",
      "  Downloading https://pypi.virtaicloud.com/repository/pypi/packages/notebook/7.2.1/notebook-7.2.1-py3-none-any.whl (5.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m214.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting qtconsole (from jupyter>=1.0.0->-r requirements.txt (line 4))\n",
      "  Downloading https://pypi.virtaicloud.com/repository/pypi/packages/qtconsole/5.5.2/qtconsole-5.5.2-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jupyter-console (from jupyter>=1.0.0->-r requirements.txt (line 4))\n",
      "  Downloading https://pypi.virtaicloud.com/repository/pypi/packages/jupyter-console/6.6.3/jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: nbconvert in /root/miniconda3/lib/python3.11/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 4)) (7.16.4)\n",
      "Requirement already satisfied: ipykernel in /root/miniconda3/lib/python3.11/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 4)) (6.29.4)\n",
      "Collecting ipywidgets (from jupyter>=1.0.0->-r requirements.txt (line 4))\n",
      "  Downloading https://pypi.virtaicloud.com/repository/pypi/packages/ipywidgets/8.1.3/ipywidgets-8.1.3-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m202.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /root/miniconda3/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (3.14.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /root/miniconda3/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /root/miniconda3/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (2.31.0)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /root/miniconda3/lib/python3.11/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 5)) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /root/miniconda3/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (0.23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (6.0.1)\n",
      "Requirement already satisfied: transformers in /root/miniconda3/lib/python3.11/site-packages (from peft>=0.10.0->-r requirements.txt (line 6)) (4.40.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /root/miniconda3/lib/python3.11/site-packages (from peft>=0.10.0->-r requirements.txt (line 6)) (0.30.1)\n",
      "Requirement already satisfied: safetensors in /root/miniconda3/lib/python3.11/site-packages (from peft>=0.10.0->-r requirements.txt (line 6)) (0.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 5)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 5)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 5)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 5)) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.19.4->datasets>=2.18.0->-r requirements.txt (line 5)) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.18.0->-r requirements.txt (line 5)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.18.0->-r requirements.txt (line 5)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.18.0->-r requirements.txt (line 5)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.18.0->-r requirements.txt (line 5)) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (1.12)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/miniconda3/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (12.4.127)\n",
      "Requirement already satisfied: comm>=0.1.1 in /root/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /root/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.8.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /root/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (8.24.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /root/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (8.6.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /root/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /root/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /root/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.6.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /root/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (26.0.3)\n",
      "Requirement already satisfied: tornado>=6.1 in /root/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (6.4)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /root/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.11 (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 4))\n",
      "  Downloading https://pypi.virtaicloud.com/repository/pypi/packages/widgetsnbextension/4.0.11/widgetsnbextension-4.0.11-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m258.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jupyterlab-widgets~=3.0.11 (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 4))\n",
      "  Downloading https://pypi.virtaicloud.com/repository/pypi/packages/jupyterlab-widgets/3.0.11/jupyterlab_widgets-3.0.11-py3-none-any.whl (214 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.4/214.4 kB\u001b[0m \u001b[31m247.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: prompt-toolkit>=3.0.30 in /root/miniconda3/lib/python3.11/site-packages (from jupyter-console->jupyter>=1.0.0->-r requirements.txt (line 4)) (3.0.43)\n",
      "Requirement already satisfied: pygments in /root/miniconda3/lib/python3.11/site-packages (from jupyter-console->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.18.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /root/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /root/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /root/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /root/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /root/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.1.5)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /root/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /root/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /root/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /root/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.5.1)\n",
      "Requirement already satisfied: tinycss2 in /root/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /root/miniconda3/lib/python3.11/site-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.14.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /root/miniconda3/lib/python3.11/site-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.27.1)\n",
      "Requirement already satisfied: jupyterlab<4.3,>=4.2.0 in /root/miniconda3/lib/python3.11/site-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (4.2.0)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /root/miniconda3/lib/python3.11/site-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.11/site-packages (from pandas->datasets>=2.18.0->-r requirements.txt (line 5)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.11/site-packages (from pandas->datasets>=2.18.0->-r requirements.txt (line 5)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.11/site-packages (from pandas->datasets>=2.18.0->-r requirements.txt (line 5)) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/miniconda3/lib/python3.11/site-packages (from pydantic->deepspeed==0.13.1->-r requirements.txt (line 7)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /root/miniconda3/lib/python3.11/site-packages (from pydantic->deepspeed==0.13.1->-r requirements.txt (line 7)) (2.18.2)\n",
      "Collecting qtpy>=2.4.0 (from qtconsole->jupyter>=1.0.0->-r requirements.txt (line 4))\n",
      "  Downloading https://pypi.virtaicloud.com/repository/pypi/packages/qtpy/2.4.1/QtPy-2.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m180.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.11/site-packages (from transformers->peft>=0.10.0->-r requirements.txt (line 6)) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/miniconda3/lib/python3.11/site-packages (from transformers->peft>=0.10.0->-r requirements.txt (line 6)) (0.19.1)\n",
      "Requirement already satisfied: webencodings in /root/miniconda3/lib/python3.11/site-packages (from bleach!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.5.1)\n",
      "Requirement already satisfied: decorator in /root/miniconda3/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /root/miniconda3/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.19.1)\n",
      "Requirement already satisfied: stack-data in /root/miniconda3/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /root/miniconda3/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (4.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /root/miniconda3/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (3.10.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /root/miniconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (4.3.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /root/miniconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /root/miniconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /root/miniconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.5.3)\n",
      "Requirement already satisfied: overrides>=5.0 in /root/miniconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /root/miniconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.20.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /root/miniconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /root/miniconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /root/miniconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.8.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /root/miniconda3/lib/python3.11/site-packages (from jupyterlab<4.3,>=4.2.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /root/miniconda3/lib/python3.11/site-packages (from jupyterlab<4.3,>=4.2.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.27.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /root/miniconda3/lib/python3.11/site-packages (from jupyterlab<4.3,>=4.2.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.2.5)\n",
      "Requirement already satisfied: babel>=2.10 in /root/miniconda3/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.15.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /root/miniconda3/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.9.25)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /root/miniconda3/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (4.22.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /root/miniconda3/lib/python3.11/site-packages (from nbformat>=5.7->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.19.1)\n",
      "Requirement already satisfied: wcwidth in /root/miniconda3/lib/python3.11/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in /root/miniconda3/lib/python3.11/site-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.11/site-packages (from sympy->torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/lib/python3.11/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /root/miniconda3/lib/python3.11/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (21.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /root/miniconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /root/miniconda3/lib/python3.11/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.8.4)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /root/miniconda3/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /root/miniconda3/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /root/miniconda3/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.18.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /root/miniconda3/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in /root/miniconda3/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /root/miniconda3/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.1.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /root/miniconda3/lib/python3.11/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /root/miniconda3/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /root/miniconda3/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /root/miniconda3/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.2.2)\n",
      "Requirement already satisfied: fqdn in /root/miniconda3/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /root/miniconda3/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /root/miniconda3/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.1)\n",
      "Requirement already satisfied: uri-template in /root/miniconda3/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /root/miniconda3/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /root/miniconda3/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /root/miniconda3/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /root/miniconda3/lib/python3.11/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /root/miniconda3/lib/python3.11/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.9.0.20240316)\n",
      "Building wheels for collected packages: deepspeed, jieba\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.13.1-py3-none-any.whl size=1350303 sha256=3a33492b8f0ee5224ba769143925f1a420483e2e115f4f1106324297b44c15de\n",
      "  Stored in directory: /root/.cache/pip/wheels/43/70/12/8a0a6d87e9601c77faa51ad9c048c35d0c099fab735418ab79\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=5e2b0dc2ab6aa1b2b17b2040732a506463e6133136c3fa10bd6fb2e9e69ecae7\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/13/a3/1085bb7139402a18fc673273370453d3d6cd075cd993032323\n",
      "Successfully built deepspeed jieba\n",
      "Installing collected packages: jieba, widgetsnbextension, ruamel.yaml.clib, rouge_chinese, qtpy, jupyterlab-widgets, ruamel_yaml, ipywidgets, deepspeed, qtconsole, peft, jupyter-console, notebook, jupyter\n",
      "  Attempting uninstall: ruamel_yaml\n",
      "    Found existing installation: ruamel.yaml 0.17.21\n",
      "    Uninstalling ruamel.yaml-0.17.21:\n",
      "      Successfully uninstalled ruamel.yaml-0.17.21\n",
      "  Attempting uninstall: deepspeed\n",
      "    Found existing installation: deepspeed 0.14.2\n",
      "    Uninstalling deepspeed-0.14.2:\n",
      "      Successfully uninstalled deepspeed-0.14.2\n",
      "Successfully installed deepspeed-0.13.1 ipywidgets-8.1.3 jieba-0.42.1 jupyter-1.0.0 jupyter-console-6.6.3 jupyterlab-widgets-3.0.11 notebook-7.2.1 peft-0.12.0 qtconsole-5.5.2 qtpy-2.4.1 rouge_chinese-1.0.3 ruamel.yaml.clib-0.2.8 ruamel_yaml-0.18.6 widgetsnbextension-4.0.11\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc52df2b-c24c-4fa2-9b67-18f3f7fb6df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.virtaicloud.com/repository/pypi/simple\n",
      "Collecting nltk\n",
      "  Downloading https://pypi.virtaicloud.com/repository/pypi/packages/nltk/3.8.1/nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m233.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /root/miniconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /root/miniconda3/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /root/miniconda3/lib/python3.11/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f50e92810011977",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1. 数据集格式解读\n",
    "\n",
    "#### 多轮对话格式\n",
    "\n",
    "多轮对话微调示例采用 ChatGLM3 对话格式约定，对不同角色添加不同 `loss_mask` 从而在一遍计算中为多轮回复计算 `loss`。\n",
    "\n",
    "对于数据文件，样例采用如下格式\n",
    "\n",
    "如果仅希望微调模型的对话能力，而非工具能力，应该按照以下格式整理数据。\n",
    "\n",
    "注意格式，最外层为一个列表，每一个字典为一组对话“conversastions”。同时一组对话中也可能有多轮问答，问答每一项有角色和内容。角色可选为用户或者助手。\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"conversations\": [\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"<system prompt text>\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"<user prompt text>\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"<assistant response text>\"\n",
    "      },\n",
    "      // ... Muti Turn\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"<user prompt text>\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"<assistant response text>\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "  // ...\n",
    "]\n",
    "```\n",
    "\n",
    "**请注意，这种方法在微调的step较多的情况下会影响到模型的工具调用功能**\n",
    "\n",
    "如果您希望微调模型的对话和工具能力，应该按照以下格式整理数据。\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"tools\": [\n",
    "      // available tools, format is not restricted\n",
    "    ],\n",
    "    \"conversations\": [\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"<system prompt text>\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"<user prompt text>\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"<assistant thought to text>\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"tool\",\n",
    "        \"name\": \"<name of the tool to be called\",\n",
    "        \"parameters\": {\n",
    "          \"<parameter_name>\": \"<parameter_value>\"\n",
    "        },\n",
    "        \"observation\": \"<observation>\"\n",
    "        // don't have to be string\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"<assistant response to observation>\"\n",
    "      },\n",
    "      // ... Muti Turn\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"<user prompt text>\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"<assistant response text>\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "  // ...\n",
    "]\n",
    "```\n",
    "\n",
    "- 关于工具描述的 system prompt 无需手动插入，预处理时会将 `tools` 字段使用 `json.dumps(..., ensure_ascii=False)`\n",
    "  格式化后插入为首条 system prompt。\n",
    "\n",
    "- 每种角色可以附带一个 `bool` 类型的 `loss` 字段，表示该字段所预测的内容是否参与 `loss`\n",
    "  计算。若没有该字段，样例实现中默认对 `system`, `user` 不计算 `loss`，其余角色则计算 `loss`。\n",
    "\n",
    "- `tool` 并不是 ChatGLM3 中的原生角色，这里的 `tool` 在预处理阶段将被自动转化为一个具有工具调用 `metadata` 的 `assistant`\n",
    "  角色（默认计算 `loss`）和一个表示工具返回值的 `observation` 角色（不计算 `loss`）。\n",
    "\n",
    "- `system` 角色为可选角色，但若存在 `system` 角色，其必须出现在 `user`\n",
    "  角色之前，且一个完整的对话数据（无论单轮或者多轮对话）只能出现一次 `system` 角色。\n",
    "\n",
    "#### 数据集格式示例\n",
    "\n",
    "这里以 AdvertiseGen 数据集为例,\n",
    "您可以从 [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) 下载 AdvertiseGen 数据集。\n",
    "将解压后的 AdvertiseGen 目录放到 `./data` 目录下并自行转换为如下格式数据集。\n",
    "\n",
    "> 请注意，现在的微调代码中加入了验证集，因此，对于一组完整的微调数据集，必须包含训练数据集和验证数据集，测试数据集可以不填写。或者直接用验证数据集代替。\n",
    "\n",
    "```\n",
    "{\"conversations\": [{\"role\": \"user\", \"content\": \"类型#裙*裙长#半身裙\"}, {\"role\": \"assistant\", \"content\": \"这款百搭时尚的仙女半身裙，整体设计非常的飘逸随性，穿上之后每个女孩子都能瞬间变成小仙女啦。料子非常的轻盈，透气性也很好，穿到夏天也很舒适。\"}]}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f1deb0-7c07-4e7f-8e1f-103541225d4a",
   "metadata": {},
   "source": [
    "### 2. 数据集格式转换\n",
    "\n",
    "我们使用 AdvertiseGen 数据集来进行微调。从  [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) 下载处理好的 AdvertiseGen 数据集，将解压后的 AdvertiseGen 目录放到本目录的 `./data/` 下, 例如。\n",
    "> /gemini/code/finetune/data\n",
    ">\n",
    "\n",
    "原始数据集AdvertiseGen的格式：\n",
    "```\n",
    "{\"content\": \"类型#上衣*材质#牛仔布*颜色#白色*风格#简约*图案#刺绣*衣样式#外套*衣款式#破洞\", \"summary\": \"简约而不简单的牛仔外套，白色的衣身十分百搭。衣身多处有做旧破洞设计，打破单调乏味，增加一丝造型看点。衣身后背处有趣味刺绣装饰，丰富层次感，彰显别样时尚。\"}\n",
    "```\n",
    "\n",
    "你需要转换后的格式：\n",
    "\n",
    "```\n",
    "{\"conversations\": [{\"role\": \"user\", \"content\": \"类型#上衣*材质#牛仔布*颜色#白色*风格#简约*图案#刺绣*衣样式#外套*衣款式#破洞\"}, {\"role\": \"assistant\", \"content\": \"简约而不简单的牛仔外套，白色的衣身十分百搭。衣身多处有做旧破洞设计，打破单调乏味，增加一丝造型看点。衣身后背处有趣味刺绣装饰，丰富层次感，彰显别样时尚。\"}]}\n",
    "```\n",
    "\n",
    "##### 动手练习1\n",
    "请动手完成以下代码补充，实现convert_adgen()方法的完整功能，将原始目录下的数据格式转换为ChatGLM3微调的指定格式，并保存至目标目录。\n",
    "\n",
    "- <1>处，加载原始数据文件中的json对象\n",
    "- <2>处，sample为转为新数据格式的一行；\n",
    "- <3>处，将sameple转为JSON格式的字符串，并确保非ASCII字符被正确处理（ensure_ascii=False）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T05:29:23.809255Z",
     "start_time": "2024-04-14T05:29:22.202731Z"
    },
    "cellView": "form",
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "def _mkdir(dir_name: Union[str, Path]):\n",
    "    dir_name = _resolve_path(dir_name)\n",
    "    if not dir_name.is_dir():\n",
    "        dir_name.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
    "    def _convert(in_file: Path, out_file: Path):\n",
    "        _mkdir(out_file.parent)\n",
    "        with open(in_file, encoding='utf-8') as fin:\n",
    "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
    "                for line in fin:\n",
    "                    dct = json.loads(line)  # <1>\n",
    "                    sample = {\n",
    "                        \"conversations\": [\n",
    "                            {\"role\": \"user\", \"content\": dct[\"content\"]},\n",
    "                            {\"role\": \"assistant\", \"content\": dct[\"summary\"]}\n",
    "                        ]\n",
    "                    }  # <2>\n",
    "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')  # <3>\n",
    "\n",
    "    data_dir = _resolve_path(data_dir)\n",
    "    save_dir = _resolve_path(save_dir)\n",
    "\n",
    "    train_file = data_dir / 'train.json'\n",
    "    if train_file.is_file():\n",
    "        out_file = save_dir / train_file.relative_to(data_dir)\n",
    "        _convert(train_file, out_file)\n",
    "\n",
    "    dev_file = data_dir / 'dev.json'\n",
    "    if dev_file.is_file():\n",
    "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
    "        _convert(dev_file, out_file)\n",
    "\n",
    "convert_adgen('data/AdvertiseGen', 'data/AdvertiseGen_fix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7a99923349056",
   "metadata": {
    "collapsed": false,
    "id": "a1b7a99923349056",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. 使用命令行进行 lora 微调\n",
    "接着，我们仅需要将配置好的参数以命令行的形式传参给程序，就可以使用命令行进行高效微调。\n",
    "\n",
    "我们采用lora微调方式，其配置文件在`configs/lora.yaml`。\n",
    "\n",
    "`lora.yaml / ptuning.yaml / sft.yaml`: 模型不同微调方式的配置文件，包括模型参数、优化器参数、训练参数等。 部分重要参数解释如下：\n",
    "\n",
    "    + data_config 部分\n",
    "        + train_file: 训练数据集的文件路径。\n",
    "        + val_file: 验证数据集的文件路径。\n",
    "        + test_file: 测试数据集的文件路径。\n",
    "        + num_proc: 在加载数据时使用的进程数量。\n",
    "    + max_input_length: 输入序列的最大长度。\n",
    "    + max_output_length: 输出序列的最大长度。\n",
    "    + training_args 部分\n",
    "        + output_dir: 用于保存模型和其他输出的目录。\n",
    "        + max_steps: 训练的最大步数。\n",
    "        + per_device_train_batch_size: 每个设备（如 GPU）的训练批次大小。\n",
    "        + dataloader_num_workers: 加载数据时使用的工作线程数量。\n",
    "        + remove_unused_columns: 是否移除数据中未使用的列。\n",
    "        + save_strategy: 模型保存策略（例如，每隔多少步保存一次）。\n",
    "        + save_steps: 每隔多少步保存一次模型。\n",
    "        + log_level: 日志级别（如 info）。\n",
    "        + logging_strategy: 日志记录策略。\n",
    "        + logging_steps: 每隔多少步记录一次日志。\n",
    "        + per_device_eval_batch_size: 每个设备的评估批次大小。\n",
    "        + evaluation_strategy: 评估策略（例如，每隔多少步进行一次评估）。\n",
    "        + eval_steps: 每隔多少步进行一次评估。\n",
    "        + predict_with_generate: 是否使用生成模式进行预测。\n",
    "    + generation_config 部分\n",
    "        + max_new_tokens: 生成的最大新 token 数量。\n",
    "    + peft_config 部分\n",
    "        + peft_type: 使用的参数有效调整类型（如 LORA）。\n",
    "        + task_type: 任务类型，这里是因果语言模型（CAUSAL_LM）。\n",
    "    + Lora 参数：\n",
    "        + r: LoRA 的秩。\n",
    "        + lora_alpha: LoRA 的缩放因子。\n",
    "        + lora_dropout: 在 LoRA 层使用的 dropout 概率\n",
    "    + P-TuningV2 参数：\n",
    "        + num_virtual_tokens: 虚拟 token 的数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02dc96b-9837-486c-81c9-f45e1e048f55",
   "metadata": {},
   "source": [
    "##### 动手练习2\n",
    "请打开`configs/lora.yaml`文件，完成以下参数设置：\n",
    "\n",
    "1. 完成设置训练数据集文件。\n",
    "2. 完成设置验证数据集文件。\n",
    "3. 完成设置测试数据集的文件，可设置为与验证数据集为同一文件。\n",
    "4. 完成设置模型保存的路径为当前目录下的`output`文件夹；\n",
    "5. 完成设置训练的步数为2000；\n",
    "6. 设置模型保存策略为`steps`，也就是按照每隔一定步数保存一次；\n",
    "7. 设置每隔500步保存一次模型；\n",
    "8. 设置参数调整的类型为`LORA`\n",
    "9. 设置peft任务类型为因果语言模型（`CAUSAL_LM`）\n",
    "10. 设置LoRA的秩为8；\n",
    "11. 设置LoRA 的缩放因子为32；\n",
    "12. 设置在 LoRA 层使用的 dropout 概率为0.1\n",
    "\n",
    "完成以上设置并保存配置文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5fbc10-6c71-4512-bed2-12b96526c777",
   "metadata": {},
   "source": [
    "##### 动手练习3\n",
    "\n",
    "补充以下代码，启动模型微调训练。\n",
    "\n",
    "- <1>处，填入输入数据文件夹的路径；\n",
    "- <2>处，填入ChatGLM3预训练模型的路径；\n",
    "- <3>处，填入训练配置文件的路径；\n",
    "\n",
    "启动训练后，注意观察输出中是否会报错，正常成功训练完预计需要40分钟到1个小时左右；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17c87410a24d844f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:23:41.282431Z",
     "start_time": "2024-04-14T05:29:23.810692Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17c87410a24d844f",
    "outputId": "e347fc7d-875e-40c9-c682-3e064100476b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config file: /etc/orion/env/env.conf\n",
      "Loading checkpoint shards:   0%|                          | 0/7 [00:00<?, ?it/s]/root/miniconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [04:04<00:00, 34.94s/it]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.0312\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 114599 examples [00:00, 297105.21 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the validation split to disable multiprocessing as it only contains one shard.\n",
      "Generating validation split: 1070 examples [00:00, 164711.91 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the test split to disable multiprocessing as it only contains one shard.\n",
      "Generating test split: 1070 examples [00:00, 141869.67 examples/s]\n",
      "Map (num_proc=16): 100%|██████| 114599/114599 [00:03<00:00, 34483.46 examples/s]\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:00<00:00, 1298.04 examples/s]\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:00<00:00, 1254.39 examples/s]\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "  0%|                                                  | 0/2000 [00:00<?, ?it/s]/root/miniconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 4.8332, 'grad_norm': 2.1732375621795654, 'learning_rate': 4.975e-05, 'epoch': 0.0}\n",
      "{'loss': 4.6098, 'grad_norm': 3.1165969371795654, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.0}\n",
      "{'loss': 4.4992, 'grad_norm': 2.9904165267944336, 'learning_rate': 4.9250000000000004e-05, 'epoch': 0.0}\n",
      "{'loss': 4.1314, 'grad_norm': 3.3989405632019043, 'learning_rate': 4.9e-05, 'epoch': 0.0}\n",
      "{'loss': 4.1186, 'grad_norm': 2.7468342781066895, 'learning_rate': 4.875e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8738, 'grad_norm': 2.947678565979004, 'learning_rate': 4.85e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8469, 'grad_norm': 2.862515449523926, 'learning_rate': 4.825e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7504, 'grad_norm': 2.9453628063201904, 'learning_rate': 4.8e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6385, 'grad_norm': 3.2044479846954346, 'learning_rate': 4.775e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7209, 'grad_norm': 3.3612380027770996, 'learning_rate': 4.75e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6764, 'grad_norm': 3.600785970687866, 'learning_rate': 4.7249999999999997e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8521, 'grad_norm': 3.837615489959717, 'learning_rate': 4.7e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6162, 'grad_norm': 3.4732460975646973, 'learning_rate': 4.6750000000000005e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7359, 'grad_norm': 4.38205623626709, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6881, 'grad_norm': 3.587327241897583, 'learning_rate': 4.6250000000000006e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7502, 'grad_norm': 3.867013692855835, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.01}\n",
      "{'loss': 3.582, 'grad_norm': 4.001464366912842, 'learning_rate': 4.575e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5826, 'grad_norm': 4.31743860244751, 'learning_rate': 4.55e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5562, 'grad_norm': 4.723599433898926, 'learning_rate': 4.525e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5826, 'grad_norm': 4.48317813873291, 'learning_rate': 4.5e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5566, 'grad_norm': 4.866414546966553, 'learning_rate': 4.4750000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6498, 'grad_norm': 4.0547404289245605, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6146, 'grad_norm': 4.6765265464782715, 'learning_rate': 4.4250000000000005e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5129, 'grad_norm': 4.499402046203613, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4773, 'grad_norm': 5.284695148468018, 'learning_rate': 4.375e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6027, 'grad_norm': 5.272326469421387, 'learning_rate': 4.35e-05, 'epoch': 0.01}\n",
      "{'loss': 3.552, 'grad_norm': 5.323275566101074, 'learning_rate': 4.325e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6145, 'grad_norm': 4.47589635848999, 'learning_rate': 4.3e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6316, 'grad_norm': 4.732390403747559, 'learning_rate': 4.275e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5426, 'grad_norm': 5.828969478607178, 'learning_rate': 4.25e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4695, 'grad_norm': 5.287674903869629, 'learning_rate': 4.2250000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6104, 'grad_norm': 5.727099895477295, 'learning_rate': 4.2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4205, 'grad_norm': 5.2246270179748535, 'learning_rate': 4.175e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4969, 'grad_norm': 5.259047031402588, 'learning_rate': 4.15e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5223, 'grad_norm': 5.503702163696289, 'learning_rate': 4.125e-05, 'epoch': 0.01}\n",
      "{'loss': 3.577, 'grad_norm': 5.169521331787109, 'learning_rate': 4.1e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3623, 'grad_norm': 4.869718551635742, 'learning_rate': 4.075e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5303, 'grad_norm': 5.118138790130615, 'learning_rate': 4.05e-05, 'epoch': 0.01}\n",
      "{'loss': 3.525, 'grad_norm': 5.235253810882568, 'learning_rate': 4.025e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4764, 'grad_norm': 5.654736042022705, 'learning_rate': 4e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6955, 'grad_norm': 5.4446187019348145, 'learning_rate': 3.9750000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5006, 'grad_norm': 5.036600112915039, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6275, 'grad_norm': 5.648343563079834, 'learning_rate': 3.9250000000000005e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4217, 'grad_norm': 6.429044723510742, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4199, 'grad_norm': 5.967890739440918, 'learning_rate': 3.875e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4291, 'grad_norm': 5.596986770629883, 'learning_rate': 3.85e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5367, 'grad_norm': 5.7274932861328125, 'learning_rate': 3.825e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4504, 'grad_norm': 6.9956583976745605, 'learning_rate': 3.8e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4664, 'grad_norm': 5.723869800567627, 'learning_rate': 3.775e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5684, 'grad_norm': 5.993565082550049, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.02}\n",
      " 25%|██████████                              | 500/2000 [07:25<24:44,  1.01it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.44s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:19<00:07,  7.85s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:33<00:00, 10.05s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.739 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 31.6479, 'eval_rouge-2': 6.88853, 'eval_rouge-l': 23.384076, 'eval_bleu-4': 0.03216603214712319, 'eval_runtime': 38.5962, 'eval_samples_per_second': 1.295, 'eval_steps_per_second': 0.104, 'epoch': 0.02}\n",
      " 25%|██████████                              | 500/2000 [08:04<24:44,  1.01it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:35<00:00, 10.05s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to output/checkpoint-500\n",
      "loading configuration file /gemini/pretrain/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "/root/miniconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.3256, 'grad_norm': 5.748398303985596, 'learning_rate': 3.7250000000000004e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5486, 'grad_norm': 6.673801898956299, 'learning_rate': 3.7e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5857, 'grad_norm': 6.003171920776367, 'learning_rate': 3.675e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4891, 'grad_norm': 5.427871227264404, 'learning_rate': 3.65e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5279, 'grad_norm': 5.431671142578125, 'learning_rate': 3.625e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6471, 'grad_norm': 5.749889850616455, 'learning_rate': 3.6e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4936, 'grad_norm': 5.773630142211914, 'learning_rate': 3.575e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3744, 'grad_norm': 5.54957389831543, 'learning_rate': 3.55e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4291, 'grad_norm': 6.2730393409729, 'learning_rate': 3.525e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4986, 'grad_norm': 6.45578670501709, 'learning_rate': 3.5e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4416, 'grad_norm': 6.223489761352539, 'learning_rate': 3.475e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4604, 'grad_norm': 6.588860034942627, 'learning_rate': 3.45e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4518, 'grad_norm': 5.994650840759277, 'learning_rate': 3.4250000000000006e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4623, 'grad_norm': 6.188559532165527, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5367, 'grad_norm': 5.884464740753174, 'learning_rate': 3.375000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4887, 'grad_norm': 6.268592834472656, 'learning_rate': 3.35e-05, 'epoch': 0.02}\n",
      "{'loss': 3.55, 'grad_norm': 6.146034240722656, 'learning_rate': 3.325e-05, 'epoch': 0.02}\n",
      "{'loss': 3.307, 'grad_norm': 6.98729133605957, 'learning_rate': 3.3e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4059, 'grad_norm': 6.607414722442627, 'learning_rate': 3.275e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3545, 'grad_norm': 6.149349689483643, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4988, 'grad_norm': 7.020326137542725, 'learning_rate': 3.2250000000000005e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5314, 'grad_norm': 6.7923455238342285, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2553, 'grad_norm': 6.927366733551025, 'learning_rate': 3.175e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5795, 'grad_norm': 5.829440593719482, 'learning_rate': 3.15e-05, 'epoch': 0.03}\n",
      "{'loss': 3.401, 'grad_norm': 6.627187252044678, 'learning_rate': 3.125e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4842, 'grad_norm': 6.092869758605957, 'learning_rate': 3.1e-05, 'epoch': 0.03}\n",
      "{'loss': 3.6252, 'grad_norm': 6.365723133087158, 'learning_rate': 3.075e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4803, 'grad_norm': 6.235572338104248, 'learning_rate': 3.05e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3289, 'grad_norm': 6.466261386871338, 'learning_rate': 3.025e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5545, 'grad_norm': 6.839439392089844, 'learning_rate': 3e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2932, 'grad_norm': 6.471981525421143, 'learning_rate': 2.975e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3617, 'grad_norm': 6.4690632820129395, 'learning_rate': 2.95e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4639, 'grad_norm': 7.229482173919678, 'learning_rate': 2.925e-05, 'epoch': 0.03}\n",
      "{'loss': 3.41, 'grad_norm': 6.332568168640137, 'learning_rate': 2.9e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5123, 'grad_norm': 6.345061302185059, 'learning_rate': 2.8749999999999997e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5406, 'grad_norm': 6.410793781280518, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2957, 'grad_norm': 7.043397903442383, 'learning_rate': 2.825e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4951, 'grad_norm': 6.571566581726074, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4576, 'grad_norm': 7.622363567352295, 'learning_rate': 2.7750000000000004e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2674, 'grad_norm': 8.222586631774902, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4625, 'grad_norm': 7.961949348449707, 'learning_rate': 2.725e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4242, 'grad_norm': 7.003200054168701, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4643, 'grad_norm': 7.6163649559021, 'learning_rate': 2.6750000000000003e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5691, 'grad_norm': 7.416588306427002, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3674, 'grad_norm': 6.5688557624816895, 'learning_rate': 2.625e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4371, 'grad_norm': 7.548030853271484, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5408, 'grad_norm': 5.979658603668213, 'learning_rate': 2.5750000000000002e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3266, 'grad_norm': 7.142567157745361, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4672, 'grad_norm': 7.4134016036987305, 'learning_rate': 2.525e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4025, 'grad_norm': 7.781716823577881, 'learning_rate': 2.5e-05, 'epoch': 0.03}\n",
      " 50%|███████████████████▌                   | 1000/2000 [15:19<15:04,  1.11it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.00s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:06<00:02,  2.11s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.899716, 'eval_rouge-2': 6.370992, 'eval_rouge-l': 25.745818000000003, 'eval_bleu-4': 0.03274703069564597, 'eval_runtime': 11.4393, 'eval_samples_per_second': 4.371, 'eval_steps_per_second': 0.35, 'epoch': 0.03}\n",
      " 50%|███████████████████▌                   | 1000/2000 [15:31<15:04,  1.11it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:08<00:00,  2.02s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to output/checkpoint-1000\n",
      "loading configuration file /gemini/pretrain/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "/root/miniconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.4508, 'grad_norm': 7.0089006423950195, 'learning_rate': 2.4750000000000002e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4645, 'grad_norm': 7.520674228668213, 'learning_rate': 2.45e-05, 'epoch': 0.04}\n",
      "{'loss': 3.657, 'grad_norm': 8.311792373657227, 'learning_rate': 2.425e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4084, 'grad_norm': 6.519405841827393, 'learning_rate': 2.4e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3918, 'grad_norm': 8.686847686767578, 'learning_rate': 2.375e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3549, 'grad_norm': 7.580915451049805, 'learning_rate': 2.35e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3932, 'grad_norm': 7.295299053192139, 'learning_rate': 2.3250000000000003e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4703, 'grad_norm': 7.252490043640137, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5322, 'grad_norm': 7.1751708984375, 'learning_rate': 2.275e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4723, 'grad_norm': 6.637091159820557, 'learning_rate': 2.25e-05, 'epoch': 0.04}\n",
      "{'loss': 3.35, 'grad_norm': 6.920156002044678, 'learning_rate': 2.2250000000000002e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5283, 'grad_norm': 8.05918025970459, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4381, 'grad_norm': 7.436394691467285, 'learning_rate': 2.175e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3645, 'grad_norm': 7.981051445007324, 'learning_rate': 2.15e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3203, 'grad_norm': 7.529036521911621, 'learning_rate': 2.125e-05, 'epoch': 0.04}\n",
      "{'loss': 3.366, 'grad_norm': 7.099845886230469, 'learning_rate': 2.1e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4551, 'grad_norm': 6.838924407958984, 'learning_rate': 2.075e-05, 'epoch': 0.04}\n",
      "{'loss': 3.477, 'grad_norm': 6.534615516662598, 'learning_rate': 2.05e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3645, 'grad_norm': 6.718226432800293, 'learning_rate': 2.025e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4148, 'grad_norm': 6.497758865356445, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2449, 'grad_norm': 6.6436896324157715, 'learning_rate': 1.9750000000000002e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3535, 'grad_norm': 7.409414291381836, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3904, 'grad_norm': 7.485225200653076, 'learning_rate': 1.925e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3801, 'grad_norm': 7.702951908111572, 'learning_rate': 1.9e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4535, 'grad_norm': 6.870307922363281, 'learning_rate': 1.8750000000000002e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2861, 'grad_norm': 7.591603755950928, 'learning_rate': 1.85e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4516, 'grad_norm': 7.355407238006592, 'learning_rate': 1.825e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3389, 'grad_norm': 7.501354694366455, 'learning_rate': 1.8e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3992, 'grad_norm': 6.9869513511657715, 'learning_rate': 1.775e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4863, 'grad_norm': 7.571056842803955, 'learning_rate': 1.75e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4701, 'grad_norm': 7.046462535858154, 'learning_rate': 1.725e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4627, 'grad_norm': 6.885220527648926, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4072, 'grad_norm': 10.06950569152832, 'learning_rate': 1.675e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3059, 'grad_norm': 7.513462066650391, 'learning_rate': 1.65e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3506, 'grad_norm': 7.421751022338867, 'learning_rate': 1.6250000000000002e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3012, 'grad_norm': 7.975094795227051, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5234, 'grad_norm': 7.340312957763672, 'learning_rate': 1.575e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3891, 'grad_norm': 7.2251973152160645, 'learning_rate': 1.55e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3576, 'grad_norm': 7.2204155921936035, 'learning_rate': 1.525e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4199, 'grad_norm': 6.837353706359863, 'learning_rate': 1.5e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3537, 'grad_norm': 7.618544578552246, 'learning_rate': 1.475e-05, 'epoch': 0.05}\n",
      "{'loss': 3.2764, 'grad_norm': 7.736716270446777, 'learning_rate': 1.45e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3855, 'grad_norm': 7.764334678649902, 'learning_rate': 1.4249999999999999e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3631, 'grad_norm': 7.474399566650391, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.05}\n",
      "{'loss': 3.2803, 'grad_norm': 7.03082799911499, 'learning_rate': 1.3750000000000002e-05, 'epoch': 0.05}\n",
      "{'loss': 3.399, 'grad_norm': 7.250433921813965, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4371, 'grad_norm': 8.957419395446777, 'learning_rate': 1.3250000000000002e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3078, 'grad_norm': 6.8156585693359375, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4455, 'grad_norm': 7.41406774520874, 'learning_rate': 1.2750000000000002e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4607, 'grad_norm': 7.019519329071045, 'learning_rate': 1.25e-05, 'epoch': 0.05}\n",
      " 75%|█████████████████████████████▎         | 1500/2000 [22:47<06:27,  1.29it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.77s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:05<00:01,  1.94s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.359608, 'eval_rouge-2': 7.47307, 'eval_rouge-l': 24.681524, 'eval_bleu-4': 0.037503943242351824, 'eval_runtime': 26.3037, 'eval_samples_per_second': 1.901, 'eval_steps_per_second': 0.152, 'epoch': 0.05}\n",
      " 75%|█████████████████████████████▎         | 1500/2000 [23:13<06:27,  1.29it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:08<00:00,  2.06s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to output/checkpoint-1500\n",
      "loading configuration file /gemini/pretrain/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "/root/miniconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 3.3539, 'grad_norm': 7.099141597747803, 'learning_rate': 1.225e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3918, 'grad_norm': 7.987817287445068, 'learning_rate': 1.2e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4414, 'grad_norm': 8.214885711669922, 'learning_rate': 1.175e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4154, 'grad_norm': 7.11168098449707, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4986, 'grad_norm': 7.454670429229736, 'learning_rate': 1.125e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4176, 'grad_norm': 8.461578369140625, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4686, 'grad_norm': 8.16973876953125, 'learning_rate': 1.075e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4443, 'grad_norm': 7.745935440063477, 'learning_rate': 1.05e-05, 'epoch': 0.06}\n",
      "{'loss': 3.517, 'grad_norm': 9.5767822265625, 'learning_rate': 1.025e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3979, 'grad_norm': 7.158376216888428, 'learning_rate': 1e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3768, 'grad_norm': 8.023505210876465, 'learning_rate': 9.750000000000002e-06, 'epoch': 0.06}\n",
      "{'loss': 3.3732, 'grad_norm': 8.558002471923828, 'learning_rate': 9.5e-06, 'epoch': 0.06}\n",
      "{'loss': 3.483, 'grad_norm': 7.595433712005615, 'learning_rate': 9.25e-06, 'epoch': 0.06}\n",
      "{'loss': 3.3262, 'grad_norm': 8.126852035522461, 'learning_rate': 9e-06, 'epoch': 0.06}\n",
      "{'loss': 3.3744, 'grad_norm': 7.538941383361816, 'learning_rate': 8.75e-06, 'epoch': 0.06}\n",
      "{'loss': 3.3111, 'grad_norm': 7.122939109802246, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.06}\n",
      "{'loss': 3.4834, 'grad_norm': 8.674233436584473, 'learning_rate': 8.25e-06, 'epoch': 0.06}\n",
      "{'loss': 3.3789, 'grad_norm': 7.431288242340088, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.06}\n",
      "{'loss': 3.3852, 'grad_norm': 7.564471244812012, 'learning_rate': 7.75e-06, 'epoch': 0.06}\n",
      "{'loss': 3.525, 'grad_norm': 7.027154922485352, 'learning_rate': 7.5e-06, 'epoch': 0.06}\n",
      "{'loss': 3.4668, 'grad_norm': 7.1374664306640625, 'learning_rate': 7.25e-06, 'epoch': 0.06}\n",
      "{'loss': 3.5139, 'grad_norm': 7.479729175567627, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.06}\n",
      "{'loss': 3.4082, 'grad_norm': 7.381870269775391, 'learning_rate': 6.750000000000001e-06, 'epoch': 0.06}\n",
      "{'loss': 3.4041, 'grad_norm': 7.785174369812012, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.06}\n",
      "{'loss': 3.4734, 'grad_norm': 7.3997931480407715, 'learning_rate': 6.25e-06, 'epoch': 0.06}\n",
      "{'loss': 3.4543, 'grad_norm': 8.100055694580078, 'learning_rate': 6e-06, 'epoch': 0.06}\n",
      "{'loss': 3.3762, 'grad_norm': 7.820586204528809, 'learning_rate': 5.750000000000001e-06, 'epoch': 0.06}\n",
      "{'loss': 3.3654, 'grad_norm': 8.144085884094238, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.06}\n",
      "{'loss': 3.3951, 'grad_norm': 8.18825626373291, 'learning_rate': 5.25e-06, 'epoch': 0.06}\n",
      "{'loss': 3.3484, 'grad_norm': 7.67244291305542, 'learning_rate': 5e-06, 'epoch': 0.06}\n",
      "{'loss': 3.3852, 'grad_norm': 9.027031898498535, 'learning_rate': 4.75e-06, 'epoch': 0.06}\n",
      "{'loss': 3.3582, 'grad_norm': 7.67410945892334, 'learning_rate': 4.5e-06, 'epoch': 0.06}\n",
      "{'loss': 3.5922, 'grad_norm': 7.975149154663086, 'learning_rate': 4.250000000000001e-06, 'epoch': 0.06}\n",
      "{'loss': 3.3578, 'grad_norm': 8.314436912536621, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.06}\n",
      "{'loss': 3.509, 'grad_norm': 9.396952629089355, 'learning_rate': 3.75e-06, 'epoch': 0.06}\n",
      "{'loss': 3.3891, 'grad_norm': 7.3737874031066895, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.06}\n",
      "{'loss': 3.3225, 'grad_norm': 8.217473983764648, 'learning_rate': 3.2500000000000002e-06, 'epoch': 0.07}\n",
      "{'loss': 3.3131, 'grad_norm': 7.33687686920166, 'learning_rate': 3e-06, 'epoch': 0.07}\n",
      "{'loss': 3.4182, 'grad_norm': 7.42085075378418, 'learning_rate': 2.7500000000000004e-06, 'epoch': 0.07}\n",
      "{'loss': 3.384, 'grad_norm': 7.585114479064941, 'learning_rate': 2.5e-06, 'epoch': 0.07}\n",
      "{'loss': 3.4008, 'grad_norm': 8.06471061706543, 'learning_rate': 2.25e-06, 'epoch': 0.07}\n",
      "{'loss': 3.4963, 'grad_norm': 7.416109561920166, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.07}\n",
      "{'loss': 3.3004, 'grad_norm': 7.751308917999268, 'learning_rate': 1.7500000000000002e-06, 'epoch': 0.07}\n",
      "{'loss': 3.5131, 'grad_norm': 7.816373825073242, 'learning_rate': 1.5e-06, 'epoch': 0.07}\n",
      "{'loss': 3.3775, 'grad_norm': 7.016813278198242, 'learning_rate': 1.25e-06, 'epoch': 0.07}\n",
      "{'loss': 3.3002, 'grad_norm': 8.631400108337402, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.07}\n",
      "{'loss': 3.3861, 'grad_norm': 7.718543529510498, 'learning_rate': 7.5e-07, 'epoch': 0.07}\n",
      "{'loss': 3.2611, 'grad_norm': 7.489039897918701, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.07}\n",
      "{'loss': 3.4328, 'grad_norm': 6.806139945983887, 'learning_rate': 2.5000000000000004e-07, 'epoch': 0.07}\n",
      "{'loss': 3.4754, 'grad_norm': 7.670791149139404, 'learning_rate': 0.0, 'epoch': 0.07}\n",
      "100%|███████████████████████████████████████| 2000/2000 [30:29<00:00,  1.19it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.68s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:06<00:02,  2.10s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.409804, 'eval_rouge-2': 7.233301999999998, 'eval_rouge-l': 25.651480000000003, 'eval_bleu-4': 0.0361631626265465, 'eval_runtime': 23.6019, 'eval_samples_per_second': 2.118, 'eval_steps_per_second': 0.169, 'epoch': 0.07}\n",
      "100%|███████████████████████████████████████| 2000/2000 [30:53<00:00,  1.19it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:20<00:00,  6.53s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to output/checkpoint-2000\n",
      "loading configuration file /gemini/pretrain/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1854.0566, 'train_samples_per_second': 4.315, 'train_steps_per_second': 1.079, 'train_loss': 3.484404296875, 'epoch': 0.07}\n",
      "100%|███████████████████████████████████████| 2000/2000 [30:54<00:00,  1.08it/s]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1070\n",
      "  Batch size = 16\n",
      "100%|███████████████████████████████████████████| 67/67 [09:40<00:00,  8.66s/it]\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 NCCL_P2P_DISABLE=\"1\" NCCL_IB_DISABLE=\"1\" python finetune_hf.py . $GEMINI_PRETRAIN configs/lora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fca985-f0e8-4195-830a-0983f8a7d69c",
   "metadata": {},
   "source": [
    "##### 从保存点进行微调\n",
    "\n",
    "如果按照上述方式进行训练，每次微调都会从头开始，如果你想从训练一半的模型开始微调，你可以加入第四个参数，这个参数有两种传入方式:\n",
    "\n",
    "1. `yes`, 自动从最后一个保存的 Checkpoint开始训练\n",
    "2. `XX`, 断点号数字 例 `600` 则从序号600 Checkpoint开始训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9418f6c5c264601",
   "metadata": {
    "collapsed": false,
    "id": "d9418f6c5c264601",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 3. 使用微调的数据集进行推理\n",
    "在完成微调任务之后，我们可以查看到 `output` 文件夹下多了很多个`checkpoint-*`的文件夹，这些文件夹代表了训练的轮数。\n",
    "我们选择最后一轮的微调权重，并使用inference进行导入。\n",
    "\n",
    "##### 动手练习4\n",
    "\n",
    "- <1>处，填入迭代轮数最大的`checkpoint-2000`文件夹的路径，\n",
    "- <2>处，填入生成广告语的提示词，提示词的写法注意参考输入训练集中提示语的写法。\n",
    "\n",
    "执行代码，观察生成的广告语。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5060015c24e97ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:23:52.725227Z",
     "start_time": "2024-04-14T06:23:41.284552Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5060015c24e97ae",
    "outputId": "d3f03d0d-46bf-4c74-9b00-dc0160da0e15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|                          | 0/7 [00:00<?, ?it/s]/root/miniconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [01:08<00:00,  9.74s/it]\n",
      "这款白色的圆领纯色短袖t恤，采用优质纯棉面料，穿着舒适柔软，吸湿透气。简约的圆领设计，穿起来不会勒脖子。简洁的白色设计，百搭不挑人，可以搭配各种下装。\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 NCCL_P2P_DISABLE=\"1\" NCCL_IB_DISABLE=\"1\" python inference_hf.py output/checkpoint-2000 --prompt \"类型#上衣*材质#棉*颜色#白色*风格#简约*图案#纯色*衣样式#T恤*衣款式#圆领\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03890258-6d9d-4042-acae-e80438c38de6",
   "metadata": {},
   "source": [
    "#### 使用微调后的模型\n",
    "\n",
    "您可以在任何一个 demo 内使用我们的 `lora` 和 全参微调的模型。这需要你自己按照以下教程进行修改代码。\n",
    "\n",
    "1. 使用`finetune_demo/inference_hf.py`中读入模型的方式替换 demo 中读入模型的方式。\n",
    "\n",
    "> 请注意，对于 LORA 和 P-TuningV2 我们没有合并训练后的模型，而是在`adapter_config.json`\n",
    "> 中记录了微调型的路径，如果你的原始模型位置发生更改，则你应该修改`adapter_config.json`中`base_model_name_or_path`的路径。\n",
    "\n",
    "```python\n",
    "def load_model_and_tokenizer(\n",
    "        model_dir: Union[str, Path], trust_remote_code: bool = True\n",
    ") -> tuple[ModelType, TokenizerType]:\n",
    "    model_dir = _resolve_path(model_dir)\n",
    "    if (model_dir / 'adapter_config.json').exists():\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            model_dir, trust_remote_code=trust_remote_code, device_map='auto'\n",
    "        )\n",
    "        tokenizer_dir = model.peft_config['default'].base_model_name_or_path\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir, trust_remote_code=trust_remote_code, device_map='auto'\n",
    "        )\n",
    "        tokenizer_dir = model_dir\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_dir, trust_remote_code=trust_remote_code\n",
    "    )\n",
    "    return model, tokenizer\n",
    "```\n",
    "\n",
    "2. 读取微调的模型，请注意，你应该使用微调模型的位置，例如，若你的模型位置为`/path/to/finetune_adapter_model`\n",
    "   ，原始模型地址为`path/to/base_model`,则你应该使用`/path/to/finetune_adapter_model`作为`model_dir`。\n",
    "3. 完成上述操作后，就能正常使用微调的模型了，其他的调用方式没有变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd83087f096094",
   "metadata": {
    "collapsed": false,
    "id": "18cd83087f096094",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 4. 总结\n",
    "到此位置，我们就完成了使用单张 GPU Lora 来微调 ChatGLM3-6B 模型，使其能生产出更好的广告。\n",
    "在本章节中，你将会学会：\n",
    "+ 如何使用模型进行 Lora 微调\n",
    "+ 微调数据集的准备和对齐\n",
    "+ 使用微调的模型进行推理"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
